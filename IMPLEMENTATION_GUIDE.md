# Project Parliament Pulse - Implementation Guide

## ðŸ›ï¸ Project Overview
A privacy-first desktop application for parliamentarians to analyze constituent email inboxes using local data processing and machine learning.

## ðŸŽ¯ MVP Scope & Architecture

### Core Principles
- **Privacy First**: All data processing occurs locally
- **Secure Access**: OAuth 2.0 only, no password storage
- **Desktop Native**: Packaged as a standalone application
- **Real-time Analysis**: Live dashboard with filtering capabilities

### Technical Stack Decision

#### Backend
- **Python 3.9+** - Core language
- **FastAPI** - Local web server and API
- **SQLite** - Local database (via sqlite3)
- **Pandas** - Data processing and SQLite interface
- **BERTopic** - Advanced topic modeling
- **vaderSentiment** - Fast sentiment analysis
- **BeautifulSoup4** - Email HTML parsing
- **Google API Client** - Gmail OAuth integration

#### Frontend (Recommended Modern Stack)
Instead of plain HTML/CSS/JS, we'll use:
- **Vite** - Modern build tool and dev server
- **TypeScript** - Type safety and better development experience
- **Tailwind CSS** - Utility-first CSS framework for clean, consistent design
- **Chart.js** - Robust charting library
- **Heroicons** - Beautiful SVG icons

This stack provides:
âœ… Modern, clean design out of the box
âœ… Excellent TypeScript support
âœ… Fast development and build times
âœ… Well-maintained and stable
âœ… Great documentation and community

#### Desktop Packaging
- **Tauri** - Rust-based desktop app framework (more secure and lighter than Electron)

## ðŸ“‹ Detailed Implementation Steps

### Phase 1: Project Foundation (Day 1)

#### Step 1.1: Directory Structure Setup
```
MP-Project/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py              # FastAPI application
â”‚   â”‚   â”œâ”€â”€ database.py          # SQLite connection & models
â”‚   â”‚   â”œâ”€â”€ email_connector.py   # OAuth & email fetching
â”‚   â”‚   â”œâ”€â”€ nlp_processor.py     # NLP pipeline
â”‚   â”‚   â””â”€â”€ config.py           # Configuration management
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ .env.example
â”‚   â””â”€â”€ run_server.py           # Development server script
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ main.ts             # Application entry point
â”‚   â”‚   â”œâ”€â”€ style.css           # Global styles
â”‚   â”‚   â”œâ”€â”€ components/         # Reusable components
â”‚   â”‚   â””â”€â”€ utils/              # Utility functions
â”‚   â”œâ”€â”€ public/
â”‚   â”‚   â””â”€â”€ index.html
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ vite.config.ts
â”‚   â”œâ”€â”€ tailwind.config.js
â”‚   â””â”€â”€ tsconfig.json
â”œâ”€â”€ src-tauri/                  # Tauri configuration (Phase 7)
â”œâ”€â”€ Model Finetuning/           # Custom ML model training
â”‚   â”œâ”€â”€ data_creation/          # Synthetic dataset generation
â”‚   â”‚   â””â”€â”€ emails_data/        # Generated email dataset (1,250 emails)
â”‚   â””â”€â”€ finetuning/             # Model training pipeline
â”‚       â”œâ”€â”€ data_preprocessing.py   # Data loading and preprocessing
â”‚       â”œâ”€â”€ topic_classifier.py     # Topic model training
â”‚       â”œâ”€â”€ sentiment_classifier.py # Sentiment model training
â”‚       â”œâ”€â”€ models/             # Saved trained models
â”‚       â”œâ”€â”€ notebooks/          # Jupyter notebooks for analysis
â”‚       â””â”€â”€ outputs/            # Training logs, plots, metrics
â”œâ”€â”€ data/                       # Local data storage
â”‚   â””â”€â”€ emails.db              # SQLite database
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ IMPLEMENTATION_GUIDE.md
```

#### Step 1.2: Backend Environment Setup
```bash
# Create Python virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

#### Step 1.3: Frontend Environment Setup
```bash
cd frontend
npm install
npm run dev  # Start development server
```

### Phase 2: Database Layer (Day 1-2)

#### Step 2.1: Database Schema Design
```sql
-- emails table
CREATE TABLE emails (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    email_id TEXT UNIQUE NOT NULL,           -- Gmail message ID
    thread_id TEXT,                          -- Gmail thread ID
    sender_email TEXT NOT NULL,
    sender_name TEXT,
    subject TEXT NOT NULL,
    received_at DATETIME NOT NULL,
    raw_body TEXT NOT NULL,                  -- Original email body
    cleaned_body TEXT NOT NULL,              -- Processed email body
    is_spam BOOLEAN DEFAULT FALSE,
    topic TEXT,                              -- Assigned topic
    topic_confidence REAL,                   -- Topic confidence score
    sentiment_label TEXT,                    -- positive/neutral/negative
    sentiment_score REAL,                    -- Numerical sentiment score
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

-- topics table (for topic management)
CREATE TABLE topics (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    name TEXT UNIQUE NOT NULL,
    description TEXT,
    color TEXT,                              -- Hex color for UI
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

-- processing_status table (for tracking sync status)
CREATE TABLE processing_status (
    id INTEGER PRIMARY KEY,
    last_sync_at DATETIME,
    emails_processed INTEGER DEFAULT 0,
    last_email_date DATETIME
);
```

#### Step 2.2: Database Connection Manager
- Implement connection pooling
- Add migration system for schema updates
- Create data access layer with Pandas integration

### Phase 3: Email Integration (Day 2-3)

#### Step 3.1: Gmail OAuth Setup
```python
# Required OAuth scopes
SCOPES = ['https://www.googleapis.com/auth/gmail.readonly']

# OAuth flow steps:
# 1. Generate authorization URL
# 2. Handle callback with authorization code
# 3. Exchange code for access token
# 4. Store refresh token securely (encrypted local file)
```

#### Step 3.2: Email Fetching Strategy
```python
# Batch processing approach:
# 1. Fetch emails in chunks (100-500 at a time)
# 2. Process incrementally to avoid API rate limits
# 3. Store raw data first, then process NLP
# 4. Implement resume capability for interrupted syncs
```

#### Step 3.3: Email Cleaning Pipeline
```python
# Multi-stage cleaning process:
# 1. HTML parsing and text extraction
# 2. Quote chain removal (regex patterns)
# 3. Signature detection and removal
# 4. Normalization (whitespace, encoding)
# 5. Language detection (English filter)
```

### Phase 4: Custom ML Model Training and NLP Pipeline (Day 3-5)

#### Step 4.1: Dataset Generation and Preprocessing
```python
# Synthetic dataset creation:
# - 1,250 labeled emails with topics, sentiment, personas
# - Multi-label topic classification (1-3 topics per email)
# - 6-class sentiment analysis (Very Negative to Very Positive, Mixed)
# - Realistic email content using LLM generation + post-processing
```

#### Step 4.2: Custom Topic Classification Model
```python
# Fine-tuned DistilBERT for topic classification:
# - Multi-label classification (emails can have multiple topics)
# - Supervised learning on domain-specific data
# - Cosine similarity evaluation for overlapping topics
# - Expected F1-score: >0.85, Cosine similarity: >0.90
```

#### Step 4.3: Custom Sentiment Analysis Model
```python
# Fine-tuned DistilBERT for sentiment analysis:
# - 6-class classification (Very Negative, Negative, Neutral, Positive, Very Positive, Mixed)
# - Supervised learning on constituent email language patterns
# - Handles formal/informal writing styles, political discourse
# - Expected accuracy: >0.90, F1-score: >0.85
```

#### Step 4.4: Model Integration and Deployment
```python
# Integration into NLP processor:
# - Load trained models for real-time inference
# - Batch processing for email sync
# - Confidence scoring and fallback handling
# - Performance optimization with model caching
```

### Phase 5: API Development (Day 4-5)

#### Step 5.1: Core API Endpoints
```python
# Authentication endpoints
POST /api/auth/gmail/start     # Initiate OAuth flow
GET  /api/auth/gmail/callback  # Handle OAuth callback
GET  /api/auth/status          # Check auth status

# Email processing endpoints
POST /api/emails/sync          # Trigger email sync
GET  /api/emails/status        # Sync progress status

# Dashboard data endpoints
GET  /api/dashboard/summary    # High-level KPIs
GET  /api/dashboard/trends     # Email volume over time
GET  /api/dashboard/topics     # Topic breakdown
GET  /api/dashboard/sentiment  # Sentiment analysis
```

#### Step 5.2: Data Aggregation Logic
```python
# Query patterns for dashboard:
# 1. Time-based grouping (day/week/month)
# 2. Topic aggregation with counts
# 3. Sentiment distribution calculations
# 4. Filtering by date ranges
# 5. Caching for performance
```

### Phase 6: Frontend Dashboard (Day 5-6)

#### Step 6.1: Component Architecture
```typescript
// Core components:
interface DashboardProps {
  dateRange: DateRange;
  granularity: 'day' | 'week' | 'month';
}

// Components:
- FilterControls      // Date picker and granularity selector
- KPICards           // Total emails, avg sentiment, topics count
- EmailTrendChart    // Line chart with Chart.js
- TopicsBarChart     // Horizontal bar chart
- SentimentTopicsChart // Grouped bar chart
- LoadingSpinner     // Processing indicator
```

#### Step 6.2: State Management
```typescript
// Simple state management with TypeScript:
interface AppState {
  isLoading: boolean;
  error: string | null;
  dateRange: { start: Date; end: Date };
  granularity: Granularity;
  dashboardData: DashboardData | null;
  authStatus: AuthStatus;
}
```

#### Step 6.3: Chart Configuration
```typescript
// Chart.js setup with TypeScript:
// - Responsive design
// - Dark/light theme support
// - Interactive tooltips
// - Export capabilities
// - Accessibility features
```

### Phase 7: Desktop Packaging with Tauri (Day 6-7)

#### Step 7.1: Tauri Setup
```bash
# Initialize Tauri
npm install --save-dev @tauri-apps/cli
npx tauri init

# Configure tauri.conf.json:
# - Set up Python backend as sidecar
# - Configure window properties
# - Set up auto-updater
# - Configure security policies
```

#### Step 7.2: Backend Integration
```rust
// Tauri commands for Python integration:
#[tauri::command]
async fn start_backend() -> Result<(), String> {
    // Start FastAPI server as background process
}

#[tauri::command]
async fn stop_backend() -> Result<(), String> {
    // Gracefully shutdown FastAPI server
}
```

#### Step 7.3: Build Configuration
```json
// tauri.conf.json configuration:
{
  "build": {
    "beforeBuildCommand": "npm run build",
    "beforeDevCommand": "npm run dev",
    "devPath": "http://localhost:3000",
    "distDir": "../frontend/dist"
  },
  "bundle": {
    "targets": ["app", "dmg", "msi"],
    "icon": ["icons/icon.png"]
  }
}
```

## ðŸš€ Development Workflow

### Daily Development Process
1. **Morning**: Plan day's objectives, review previous progress
2. **Code**: Implement features with test-driven approach
3. **Test**: Manual testing of new features
4. **Evening**: Commit progress, update documentation

### Testing Strategy
- **Unit Tests**: Core NLP functions and data processing
- **Integration Tests**: API endpoints and database operations
- **Manual Testing**: UI interactions and email processing
- **Performance Testing**: Large email dataset processing

### Security Considerations
- **OAuth Tokens**: Encrypted storage using system keyring
- **Local Database**: File permissions and encryption at rest
- **API Security**: CORS configuration and rate limiting
- **Desktop App**: Code signing and secure update mechanism

## ðŸ“Š Success Metrics for MVP

### Functional Requirements
âœ… Successfully authenticate with Gmail OAuth
âœ… Import and process 1000+ emails without errors
âœ… Generate accurate topic classifications
âœ… Display interactive dashboard with all charts
âœ… Package as standalone desktop application

### Performance Requirements
âœ… Email sync: Process 100 emails per minute
âœ… Dashboard load: Display data within 2 seconds
âœ… Topic modeling: Complete within 30 seconds for 1000 emails
âœ… Memory usage: Under 500MB for typical usage

### User Experience Requirements
âœ… Clean, intuitive interface design
âœ… Responsive interactions (no UI freezing)
âœ… Clear error messages and progress indicators
âœ… Offline capability after initial sync

## ðŸ”„ Future Enhancements (Post-MVP)

1. **Advanced NLP Features**
   - Named Entity Recognition (NER)
   - Intent classification
   - Multi-language support

2. **Enhanced Analytics**
   - Constituency mapping
   - Trend predictions
   - Comparative analysis

3. **Integration Expansion**
   - Microsoft 365 support
   - Social media monitoring
   - Calendar integration

4. **Collaboration Features**
   - Team dashboards
   - Report generation
   - Data export capabilities

This implementation guide provides a comprehensive roadmap for building the MVP systematically while maintaining code quality and user experience standards. 