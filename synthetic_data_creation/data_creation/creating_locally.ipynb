{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "090d88a1",
   "metadata": {},
   "source": [
    "This notebook will be used to create a synthetic dataset locally with gpt-oss 20b. It will also feature determenisticly produced datestampts to be able to showcase the analytical capabilities of the POC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e65a85ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency trends per topic: {'Healthcare': 'flat', 'Education': 'flat', 'Crime': 'down', 'Immigration': 'up', 'Transportation': 'down', 'Environment': 'flat', 'Taxation': 'up', 'Housing': 'up', 'Pensions': 'down'}\n",
      "Sentiment trends per topic: {'Healthcare': 'down', 'Education': 'flat', 'Crime': 'up', 'Immigration': 'down', 'Transportation': 'flat', 'Environment': 'up', 'Taxation': 'down', 'Housing': 'up', 'Pensions': 'flat'}\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports, configuration, and topic pattern assignment\n",
    "\n",
    "import os\n",
    "import math\n",
    "import uuid\n",
    "import json\n",
    "import random\n",
    "import asyncio\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import httpx\n",
    "\n",
    "# -----------------------------\n",
    "# Reproducibility\n",
    "# -----------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# -----------------------------\n",
    "# Core configuration\n",
    "# -----------------------------\n",
    "TOTAL_EMAILS = 2500\n",
    "DAYS = 60  # ~2 months\n",
    "ENDS_TODAY = True  # if False, set FIXED_END_DATE below\n",
    "FIXED_END_DATE = datetime(2025, 6, 30).date()  # used only if ENDS_TODAY=False\n",
    "\n",
    "# Ollama (local LLM) configuration\n",
    "OLLAMA_BASE_URL = os.environ.get(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n",
    "LLM_MODEL = os.environ.get(\"LLM_MODEL\", \"gpt-oss:20b\")\n",
    "CONCURRENCY = 8  # tune based on your machine\n",
    "GEN_TIMEOUT = 90  # seconds\n",
    "\n",
    "# Output\n",
    "OUTPUT_DIR = os.path.abspath(\"ModelFinetuning/data_creation/emails_data\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "OUTPUT_CSV = os.path.join(OUTPUT_DIR, \"email_dataset_2500_with_timestamps.csv\")\n",
    "\n",
    "# Topics (exactly one topic per email)\n",
    "TOPICS = [\n",
    "    \"Healthcare\",\n",
    "    \"Education\",\n",
    "    \"Crime\",\n",
    "    \"Immigration\",\n",
    "    \"Transportation\",\n",
    "    \"Environment\",\n",
    "    \"Taxation\",\n",
    "    \"Housing\",\n",
    "    \"Pensions\",\n",
    "]\n",
    "\n",
    "# Sentiments\n",
    "SENTIMENTS = [\"Very Negative\", \"Negative\", \"Neutral\", \"Positive\", \"Very Positive\"]\n",
    "\n",
    "# Length distribution (optional metadata)\n",
    "LENGTH_BUCKETS = [\"Short\", \"Medium\", \"Long\"]\n",
    "LENGTH_PROBS = [0.10, 0.80, 0.10]\n",
    "LENGTH_TO_WORDS = {\"Short\": (60, 120), \"Medium\": (130, 280), \"Long\": (300, 600)}\n",
    "\n",
    "# -----------------------------\n",
    "# Randomly assign frequency trends: 3 up, 3 down, 3 flat (constant-ish)\n",
    "# -----------------------------\n",
    "topics_shuffled = TOPICS.copy()\n",
    "random.shuffle(topics_shuffled)\n",
    "freq_up = topics_shuffled[:3]\n",
    "freq_down = topics_shuffled[3:6]\n",
    "freq_flat = topics_shuffled[6:]\n",
    "\n",
    "FREQ_TREND = {t: (\"up\" if t in freq_up else \"down\" if t in freq_down else \"flat\") for t in TOPICS}\n",
    "\n",
    "# -----------------------------\n",
    "# Randomly assign sentiment trends (independent of frequency):\n",
    "# 3 up (towards more negative), 3 down (towards more positive), 3 flat\n",
    "# -----------------------------\n",
    "topics_shuffled2 = TOPICS.copy()\n",
    "random.shuffle(topics_shuffled2)\n",
    "sent_up = topics_shuffled2[:3]      # \"up\" means more negative over time\n",
    "sent_down = topics_shuffled2[3:6]   # \"down\" means more positive over time\n",
    "sent_flat = topics_shuffled2[6:]\n",
    "\n",
    "SENT_TREND = {t: (\"up\" if t in sent_up else \"down\" if t in sent_down else \"flat\") for t in TOPICS}\n",
    "\n",
    "print(\"Frequency trends per topic:\", FREQ_TREND)\n",
    "print(\"Sentiment trends per topic:\", SENT_TREND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a237191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Helpers for trends, seasonality, spikes, and daily totals\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def make_trend(kind: str, days: int) -> np.ndarray:\n",
    "    x = np.linspace(-2.0, 2.0, days)\n",
    "    if kind == \"up\":\n",
    "        base = sigmoid(x)               # gently increasing\n",
    "    elif kind == \"down\":\n",
    "        base = 1.0 - sigmoid(x)         # gently decreasing\n",
    "    else:\n",
    "        base = np.ones(days) * 0.6      # constant-ish baseline\n",
    "        base += np.random.normal(0.0, 0.03, size=days)  # tiny wobble\n",
    "    base = np.clip(base, 0.1, None)\n",
    "    return base\n",
    "\n",
    "def weekly_seasonality(days: int, weekend_factor: float = 0.8) -> np.ndarray:\n",
    "    # lower activity on weekends\n",
    "    if ENDS_TODAY:\n",
    "        end_date = datetime.now().date()\n",
    "    else:\n",
    "        end_date = FIXED_END_DATE\n",
    "    start_date = end_date - timedelta(days=days - 1)\n",
    "    vals = []\n",
    "    for d in range(days):\n",
    "        date = start_date + timedelta(days=d)\n",
    "        dow = date.weekday()  # Mon=0\n",
    "        vals.append(weekend_factor if dow >= 5 else 1.0)\n",
    "    return np.array(vals)\n",
    "\n",
    "def smooth_wave(days: int, amplitude: float = 0.15, period: float = 30.0, phase: float = 0.5) -> np.ndarray:\n",
    "    x = np.arange(days)\n",
    "    return 1.0 + amplitude * np.sin(2 * np.pi * x / period + phase)\n",
    "\n",
    "def add_spikes(base: np.ndarray, num_spikes: int = 2, max_amp: float = 0.8, width_range: Tuple[float, float] = (2.0, 6.0)) -> np.ndarray:\n",
    "    x = np.arange(len(base))\n",
    "    spikes = np.zeros_like(base)\n",
    "    for _ in range(num_spikes):\n",
    "        center = np.random.randint(int(0.1 * len(base)), int(0.9 * len(base)))\n",
    "        width = np.random.uniform(*width_range)\n",
    "        amp = np.random.uniform(0.2, max_amp)\n",
    "        spikes += amp * np.exp(-0.5 * ((x - center) / width) ** 2)\n",
    "    return base + spikes\n",
    "\n",
    "def normalize_rows_to_probs(M: np.ndarray) -> np.ndarray:\n",
    "    row_sums = M.sum(axis=1, keepdims=True)\n",
    "    out = np.divide(M, row_sums, out=np.ones_like(M) / M.shape[1], where=row_sums != 0)\n",
    "    return out\n",
    "\n",
    "def integer_allocation_from_probs(total: int, probs: np.ndarray) -> np.ndarray:\n",
    "    raw = probs * total\n",
    "    ints = np.floor(raw).astype(int)\n",
    "    remainder = total - ints.sum()\n",
    "    order = np.argsort(-(raw - ints))\n",
    "    for i in order[:remainder]:\n",
    "        ints[i] += 1\n",
    "    return ints\n",
    "\n",
    "def build_day_totals(days: int, total_emails: int) -> np.ndarray:\n",
    "    # daily volume with seasonality + slow wave + light noise\n",
    "    w = weekly_seasonality(days, weekend_factor=0.8)\n",
    "    wave = smooth_wave(days, amplitude=0.12, period=28.0, phase=0.7)\n",
    "    noise = np.random.normal(1.0, 0.04, size=days)\n",
    "    base = w * wave * noise\n",
    "    base = np.clip(base, 0.4, None)\n",
    "    base = base / base.sum()\n",
    "    raw = base * total_emails\n",
    "    ints = np.floor(raw).astype(int)\n",
    "    remainder = total_emails - ints.sum()\n",
    "    order = np.argsort(-(raw - ints))\n",
    "    for i in order[:remainder]:\n",
    "        ints[i] += 1\n",
    "    return ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d689e506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check totals: 2500 == 2500\n",
      "Total emails per topic: {'Healthcare': 292, 'Education': 275, 'Crime': 289, 'Immigration': 261, 'Transportation': 255, 'Environment': 330, 'Taxation': 268, 'Housing': 255, 'Pensions': 275}\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Build per-topic daily frequencies and per-topic daily sentiment \"negativity index\"\n",
    "\n",
    "# Frequency intensity per topic per day\n",
    "freq_matrix = []\n",
    "for t in TOPICS:\n",
    "    trend = make_trend(FREQ_TREND[t], DAYS)\n",
    "    seas = weekly_seasonality(DAYS, weekend_factor=0.85)\n",
    "    wave = smooth_wave(DAYS, amplitude=0.10, period=26.0, phase=np.random.uniform(0, 2*np.pi))\n",
    "    base = trend * seas * wave\n",
    "    base += np.random.normal(0.0, 0.03, size=DAYS)  # light noise\n",
    "    base = np.clip(base, 0.05, None)\n",
    "    base = add_spikes(base, num_spikes=2, max_amp=0.7, width_range=(2.5, 6.0))  # spikes in topic frequency\n",
    "    freq_matrix.append(base)\n",
    "\n",
    "freq_matrix = np.stack(freq_matrix, axis=1)  # shape: (DAYS, N_TOPICS)\n",
    "# Normalize per day to probabilities over topics\n",
    "topic_probs_per_day = normalize_rows_to_probs(freq_matrix)\n",
    "\n",
    "# Daily totals\n",
    "day_totals = build_day_totals(DAYS, TOTAL_EMAILS)\n",
    "\n",
    "# Allocate integer counts per day per topic\n",
    "day_topic_counts = np.zeros_like(topic_probs_per_day, dtype=int)\n",
    "for d in range(DAYS):\n",
    "    day_topic_counts[d] = integer_allocation_from_probs(day_totals[d], topic_probs_per_day[d])\n",
    "\n",
    "# Sentiment \"negativity index\" per topic per day (0..1), with its own trend + spikes\n",
    "neg_index = []\n",
    "for t in TOPICS:\n",
    "    kind = SENT_TREND[t]\n",
    "    if kind == \"up\":\n",
    "        base = make_trend(\"up\", DAYS)  # more negative over time\n",
    "    elif kind == \"down\":\n",
    "        base = make_trend(\"down\", DAYS)  # more positive over time\n",
    "    else:\n",
    "        base = make_trend(\"flat\", DAYS)\n",
    "    base = add_spikes(base, num_spikes=2, max_amp=0.5, width_range=(2.0, 5.0))  # spikes in sentiment\n",
    "    base += np.random.normal(0.0, 0.03, size=DAYS)\n",
    "    base = np.clip(base, 0.0, 1.0)\n",
    "    neg_index.append(base)\n",
    "\n",
    "neg_index = np.stack(neg_index, axis=1)  # shape: (DAYS, N_TOPICS)\n",
    "\n",
    "print(\"Check totals:\", day_topic_counts.sum(), \"==\", TOTAL_EMAILS)\n",
    "topic_totals = dict(zip(TOPICS, day_topic_counts.sum(axis=0).tolist()))\n",
    "print(\"Total emails per topic:\", topic_totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34170fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Sampling utilities for sentiment/length, timestamp assignment, and prompt building\n",
    "\n",
    "def sentiment_probs_from_neg_index(neg_val: float) -> np.ndarray:\n",
    "    # Map 0..1 negativity to 5-way sentiment distribution\n",
    "    # More mass to positive when neg_val low; more to negative when neg_val high.\n",
    "    neg_val = float(np.clip(neg_val, 0.0, 1.0))\n",
    "    very_neg = 0.05 + 0.35 * (neg_val ** 1.2)\n",
    "    neg = 0.10 + 0.30 * (neg_val ** 1.0)\n",
    "    very_pos = 0.05 + 0.30 * ((1 - neg_val) ** 1.2)\n",
    "    pos = 0.10 + 0.25 * ((1 - neg_val) ** 1.0)\n",
    "    neutral = 1.0 - (very_neg + neg + pos + very_pos)\n",
    "    probs = np.array([very_neg, neg, neutral, pos, very_pos])\n",
    "    probs = np.clip(probs, 0.0, None)\n",
    "    probs = probs / probs.sum()\n",
    "    return probs\n",
    "\n",
    "def sample_sentiment_for(day: int, topic_idx: int) -> str:\n",
    "    p = sentiment_probs_from_neg_index(neg_index[day, topic_idx])\n",
    "    return np.random.choice(SENTIMENTS, p=p)\n",
    "\n",
    "def sample_length() -> str:\n",
    "    return np.random.choice(LENGTH_BUCKETS, p=LENGTH_PROBS)\n",
    "\n",
    "def sample_word_count(bucket: str) -> int:\n",
    "    lo, hi = LENGTH_TO_WORDS[bucket]\n",
    "    return np.random.randint(lo, hi + 1)\n",
    "\n",
    "def assign_timestamps(days: int, counts_per_day: np.ndarray) -> List[str]:\n",
    "    # spread within each day mostly 09:00-20:00\n",
    "    if ENDS_TODAY:\n",
    "        end_date = datetime.now().date()\n",
    "    else:\n",
    "        end_date = FIXED_END_DATE\n",
    "    start_date = end_date - timedelta(days=days - 1)\n",
    "    out = []\n",
    "    for d in range(days):\n",
    "        c = int(counts_per_day[d])\n",
    "        for _ in range(c):\n",
    "            minute = np.random.randint(9 * 60, 20 * 60)\n",
    "            dt = datetime.combine(start_date + timedelta(days=d), datetime.min.time()) + timedelta(minutes=minute)\n",
    "            out.append(dt.isoformat())\n",
    "    return out\n",
    "\n",
    "def build_prompt(topic: str, sentiment: str, words: int) -> str:\n",
    "    tone = {\n",
    "        \"Very Negative\": \"The tone should be very negative and strongly critical.\",\n",
    "        \"Negative\": \"The tone should be negative and concerned.\",\n",
    "        \"Neutral\": \"The tone should be neutral and factual.\",\n",
    "        \"Positive\": \"The tone should be positive and supportive.\",\n",
    "        \"Very Positive\": \"The tone should be very positive and highly supportive.\",\n",
    "    }[sentiment]\n",
    "\n",
    "    return f\"\"\"\n",
    "Write a realistic UK constituent email to their MP.\n",
    "\n",
    "Requirements:\n",
    "- Audience: MP Mr Miller. Start with \"Dear Mr Miller,\" and end with a short sign-off and the sender's first name only.\n",
    "- Topic focus: {topic}. Keep the content tightly focused on this one topic.\n",
    "- Tone: {tone}\n",
    "- Length: about {words} words.\n",
    "- Style: UK English with concrete, grounded details (local experiences, practical concerns), no generic filler.\n",
    "- Do NOT include dates, subject lines, or any headers apart from greeting/sign-off.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c5110d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felixmul/Desktop/repos/MP-Project/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using concurrency: 8 (override with GEN_CONCURRENCY env var)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating (conc=8):   8%|▊         | 200/2500 [32:41<6:01:49,  9.44s/it, 0.10 e/s | ETA 376.0 min | fails 19]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress 200/2500 | Topic done counts: Healthcare:31, Education:26, Crime:32, Immigration:5, Transportation:33, Environment:23, Taxation:9, Housing:7, Pensions:34\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating (conc=8):  16%|█▌        | 400/2500 [1:04:55<4:58:18,  8.52s/it, 0.10 e/s | ETA 340.8 min | fails 24]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress 400/2500 | Topic done counts: Healthcare:55, Education:47, Crime:61, Immigration:12, Transportation:65, Environment:46, Taxation:22, Housing:23, Pensions:69\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating (conc=8):  24%|██▍       | 600/2500 [2:29:22<25:13:03, 47.78s/it, 0.10 e/s | ETA 306.8 min | fails 38]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress 600/2500 | Topic done counts: Healthcare:85, Education:70, Crime:86, Immigration:20, Transportation:91, Environment:68, Taxation:34, Housing:36, Pensions:110\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating (conc=8):  32%|███▏      | 800/2500 [3:01:46<4:31:12,  9.57s/it, 0.10 e/s | ETA 274.7 min | fails 53] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress 800/2500 | Topic done counts: Healthcare:104, Education:91, Crime:110, Immigration:36, Transportation:113, Environment:102, Taxation:49, Housing:50, Pensions:145\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating (conc=8):  40%|████      | 1000/2500 [3:34:57<4:21:22, 10.46s/it, 0.10 e/s | ETA 243.7 min | fails 71]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress 1000/2500 | Topic done counts: Healthcare:127, Education:118, Crime:142, Immigration:61, Transportation:131, Environment:130, Taxation:59, Housing:62, Pensions:170\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating (conc=8):  48%|████▊     | 1200/2500 [4:09:15<4:00:29, 11.10s/it, 0.10 e/s | ETA 213.2 min | fails 105]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress 1200/2500 | Topic done counts: Healthcare:147, Education:141, Crime:172, Immigration:89, Transportation:148, Environment:154, Taxation:72, Housing:73, Pensions:204\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating (conc=8):  56%|█████▌    | 1400/2500 [4:43:07<3:03:40, 10.02s/it, 0.10 e/s | ETA 181.2 min | fails 136]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress 1400/2500 | Topic done counts: Healthcare:167, Education:163, Crime:203, Immigration:114, Transportation:166, Environment:178, Taxation:91, Housing:87, Pensions:231\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating (conc=8):  64%|██████▍   | 1600/2500 [5:17:07<2:19:33,  9.30s/it, 0.10 e/s | ETA 148.9 min | fails 151]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress 1600/2500 | Topic done counts: Healthcare:183, Education:183, Crime:232, Immigration:138, Transportation:188, Environment:214, Taxation:112, Housing:105, Pensions:245\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating (conc=8):  72%|███████▏  | 1800/2500 [5:50:20<1:51:29,  9.56s/it, 0.10 e/s | ETA 115.8 min | fails 169]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress 1800/2500 | Topic done counts: Healthcare:200, Education:207, Crime:259, Immigration:156, Transportation:212, Environment:245, Taxation:137, Housing:128, Pensions:256\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating (conc=8):  80%|████████  | 2000/2500 [6:23:58<1:07:32,  8.10s/it, 0.10 e/s | ETA 82.9 min | fails 193] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress 2000/2500 | Topic done counts: Healthcare:223, Education:225, Crime:274, Immigration:184, Transportation:235, Environment:273, Taxation:170, Housing:154, Pensions:262\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating (conc=8):  88%|████████▊ | 2200/2500 [6:58:24<55:04, 11.02s/it, 0.10 e/s | ETA 49.9 min | fails 217]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress 2200/2500 | Topic done counts: Healthcare:257, Education:244, Crime:281, Immigration:211, Transportation:245, Environment:295, Taxation:204, Housing:195, Pensions:268\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating (conc=8):  96%|█████████▌| 2400/2500 [7:32:12<17:24, 10.45s/it, 0.10 e/s | ETA 16.7 min | fails 243]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress 2400/2500 | Topic done counts: Healthcare:281, Education:263, Crime:286, Immigration:243, Transportation:252, Environment:318, Taxation:247, Housing:237, Pensions:273\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating (conc=8): 100%|██████████| 2500/2500 [7:48:55<00:00, 11.25s/it, 0.10 e/s | ETA 0.0 min | fails 252] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress 2500/2500 | Topic done counts: Healthcare:292, Education:275, Crime:289, Immigration:261, Transportation:255, Environment:330, Taxation:268, Housing:255, Pensions:275\n",
      "\n",
      "Assembling DataFrame...\n",
      "Saving CSV...\n",
      "Saved 2500 emails to /Users/felixmul/Desktop/repos/MP-Project/ModelFinetuning/data_creation/ModelFinetuning/data_creation/emails_data/email_dataset_2500_with_timestamps.csv | failures during generation: 252\n",
      "\n",
      "Per-topic totals:\n",
      "topic_gt\n",
      "Environment       330\n",
      "Healthcare        292\n",
      "Crime             289\n",
      "Education         275\n",
      "Pensions          275\n",
      "Taxation          268\n",
      "Immigration       261\n",
      "Housing           255\n",
      "Transportation    255\n",
      "dtype: int64\n",
      "\n",
      "Sentiment distribution:\n",
      "sentiment_gt\n",
      "Negative         702\n",
      "Very Negative    583\n",
      "Positive         501\n",
      "Very Positive    378\n",
      "Neutral          336\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample rows:\n",
      "                               email_id            timestamp         to  \\\n",
      "0  23d27c17-9d1d-47ca-8387-f08f21fe0162  2025-06-29T12:24:00  Mr Miller   \n",
      "1  d8de6bd2-da71-4531-9f72-3fd15d869767  2025-06-29T18:41:00  Mr Miller   \n",
      "2  2da36f03-2388-464f-aa4e-af84c6630786  2025-06-29T18:15:00  Mr Miller   \n",
      "\n",
      "     topic_gt   sentiment_gt length_bucket  word_target  \\\n",
      "0  Healthcare  Very Negative        Medium          259   \n",
      "1  Healthcare  Very Negative        Medium          208   \n",
      "2  Healthcare  Very Negative          Long          362   \n",
      "\n",
      "                                          email_text  \n",
      "0                                    Dear Mr Miller,  \n",
      "1           Dear Mr Miller,\\n\\n[Generation failed: ]  \n",
      "2  Dear Mr Miller,\\n\\nI am writing to express my ...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Build records, generate emails via Ollama (async), faster settings + live progress, and save CSV\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import os\n",
    "import httpx\n",
    "\n",
    "def estimate_max_tokens(words: int) -> int:\n",
    "    # ~1.3 tokens/word; add headroom but cap to keep fast\n",
    "    return int(max(96, min(384, round(words * 1.4))))\n",
    "\n",
    "async def call_ollama_generate(prompt: str, client: httpx.AsyncClient, max_tokens: int, timeout: int = GEN_TIMEOUT) -> str:\n",
    "    payload = {\n",
    "        \"model\": LLM_MODEL,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"keep_alive\": \"30m\",  # keep model hot\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.4,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 50,\n",
    "            \"num_predict\": int(max_tokens),\n",
    "            \"num_ctx\": 1024,                  # smaller context = faster\n",
    "            \"num_thread\": os.cpu_count() or 8 # use all CPU threads (Metal will use GPU)\n",
    "        },\n",
    "    }\n",
    "    r = await client.post(f\"{OLLAMA_BASE_URL}/api/generate\", json=payload, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    return data.get(\"response\", \"\").strip()\n",
    "\n",
    "# Flatten day_topic_counts into per-email rows while preserving day order\n",
    "rows_meta = []\n",
    "for d in range(DAYS):\n",
    "    for j, topic in enumerate(TOPICS):\n",
    "        cnt = int(day_topic_counts[d, j])\n",
    "        for _ in range(cnt):\n",
    "            rows_meta.append({\"day\": d, \"topic\": topic, \"topic_idx\": j})\n",
    "\n",
    "# Assign timestamps (one per email in day order)\n",
    "timestamps = assign_timestamps(DAYS, day_totals)\n",
    "\n",
    "# Build prompts and per-email sentiment/length\n",
    "prompts = []\n",
    "email_meta = []\n",
    "max_tokens_list = []\n",
    "for i, r in enumerate(rows_meta):\n",
    "    sent = sample_sentiment_for(r[\"day\"], r[\"topic_idx\"])\n",
    "    length_bucket = sample_length()\n",
    "    words = sample_word_count(length_bucket)\n",
    "    prompt = build_prompt(r[\"topic\"], sent, words)\n",
    "    prompts.append(prompt)\n",
    "    email_meta.append({\n",
    "        \"email_id\": str(uuid.uuid4()),\n",
    "        \"timestamp\": timestamps[i],\n",
    "        \"to\": \"Mr Miller\",\n",
    "        \"topic_gt\": r[\"topic\"],\n",
    "        \"sentiment_gt\": sent,\n",
    "        \"length_bucket\": length_bucket,\n",
    "        \"word_target\": words,\n",
    "    })\n",
    "    max_tokens_list.append(estimate_max_tokens(words))\n",
    "\n",
    "# Async generation with tuned concurrency + live progress\n",
    "EFFECTIVE_CONCURRENCY = int(os.environ.get(\"GEN_CONCURRENCY\", str(CONCURRENCY)))\n",
    "semaphore = asyncio.Semaphore(EFFECTIVE_CONCURRENCY)\n",
    "results = [\"\"] * len(prompts)\n",
    "topic_counts_done = {t: 0 for t in TOPICS}\n",
    "stats = {\"fail\": 0}\n",
    "counter_lock = asyncio.Lock()\n",
    "\n",
    "async def worker(idx: int, client: httpx.AsyncClient):\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            text = await call_ollama_generate(prompts[idx], client, max_tokens=max_tokens_list[idx])\n",
    "            if not text.lstrip().startswith(\"Dear Mr Miller\"):\n",
    "                text = \"Dear Mr Miller,\\n\\n\" + text.strip()\n",
    "            results[idx] = text.strip()\n",
    "        except Exception as e:\n",
    "            results[idx] = f\"Dear Mr Miller,\\n\\n[Generation failed: {e}]\"\n",
    "            async with counter_lock:\n",
    "                stats[\"fail\"] += 1\n",
    "        finally:\n",
    "            async with counter_lock:\n",
    "                topic_counts_done[email_meta[idx][\"topic_gt\"]] += 1\n",
    "\n",
    "async def generate_all():\n",
    "    total = len(prompts)\n",
    "    start_t = time.perf_counter()\n",
    "    limits = httpx.Limits(max_connections=EFFECTIVE_CONCURRENCY, max_keepalive_connections=EFFECTIVE_CONCURRENCY)\n",
    "    async with httpx.AsyncClient(limits=limits) as client:\n",
    "        tasks = [asyncio.create_task(worker(i, client)) for i in range(total)]\n",
    "        with tqdm(total=total, desc=f\"Generating (conc={EFFECTIVE_CONCURRENCY})\", smoothing=0.2) as pbar:\n",
    "            for fut in asyncio.as_completed(tasks):\n",
    "                await fut\n",
    "                done = pbar.n + 1\n",
    "                elapsed = time.perf_counter() - start_t\n",
    "                rate = done / max(elapsed, 1e-6)\n",
    "                eta = (total - done) / max(rate, 1e-6)\n",
    "                pbar.set_postfix_str(f\"{rate:.2f} e/s | ETA {eta/60:.1f} min | fails {stats['fail']}\")\n",
    "                pbar.update(1)\n",
    "                if done % 200 == 0 or done == total:\n",
    "                    snapshot = \", \".join(f\"{k}:{topic_counts_done[k]}\" for k in TOPICS)\n",
    "                    print(f\"\\nProgress {done}/{total} | Topic done counts: {snapshot}\\n\")\n",
    "\n",
    "print(f\"Using concurrency: {EFFECTIVE_CONCURRENCY} (override with GEN_CONCURRENCY env var)\")\n",
    "await generate_all()\n",
    "\n",
    "# Build DataFrame and save\n",
    "print(\"Assembling DataFrame...\")\n",
    "df = pd.DataFrame({\n",
    "    \"email_id\": [m[\"email_id\"] for m in email_meta],\n",
    "    \"timestamp\": [m[\"timestamp\"] for m in email_meta],\n",
    "    \"to\": [m[\"to\"] for m in email_meta],\n",
    "    \"topic_gt\": [m[\"topic_gt\"] for m in email_meta],\n",
    "    \"sentiment_gt\": [m[\"sentiment_gt\"] for m in email_meta],\n",
    "    \"length_bucket\": [m[\"length_bucket\"] for m in email_meta],\n",
    "    \"word_target\": [m[\"word_target\"] for m in email_meta],\n",
    "    \"email_text\": results,\n",
    "})\n",
    "print(\"Saving CSV...\")\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"Saved {len(df)} emails to {OUTPUT_CSV} | failures during generation: {stats['fail']}\")\n",
    "\n",
    "# Quick sanity summaries\n",
    "print(\"\\nPer-topic totals:\")\n",
    "print(df.groupby(\"topic_gt\").size().sort_values(ascending=False))\n",
    "print(\"\\nSentiment distribution:\")\n",
    "print(df[\"sentiment_gt\"].value_counts())\n",
    "print(\"\\nSample rows:\")\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92ce560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785a415a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
